{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Code VERY SLIGHTLY adapted from Jawahar et al. (2019) to work with DistilBert instead of Bert"
      ],
      "metadata": {
        "id": "iGPrbbT1JoTZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w69xWr4GRsU6"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aw8V6tnSg5Jv",
        "outputId": "b441019d-66f9-4a45-c342-0aa13823e72f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPk8jrLtyc6Q",
        "outputId": "39215388-1147-4164-f431-0036569faefa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/DeepLearningAdvanced/interpret_bert-master/probing\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/DeepLearningAdvanced/interpret_bert-master/probing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each task, extract trained embeddings, and untrained with --untrained_bert flag"
      ],
      "metadata": {
        "id": "7umfZPUQJTdn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The task files are bigram_shift.txt, coordination_inversion.txt, obj_number.txt, odd_man_out.txt (SOMO in the paper), past_present.txt (Tense in the paper), sentence_length.txt, subj_number.txt, top_constituents.txt, tree_depth.txt and word_content.txt"
      ],
      "metadata": {
        "id": "MYWy19clNenx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZdslGyiz7gs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76488743-c114-42b1-94c7-a94780df0d68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(batch_size=8, cache_dir='/tmp', data_file='sentence_length_small.txt', no_cuda=False, num_gpus=1, output_file='sentence_length_short_untrained.json', untrained_bert=True)\n",
            "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "extract_features.py:166: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  torch.nn.init.xavier_uniform(m.weight)\n",
            "extract_features.py:161: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  torch.nn.init.xavier_uniform(m.weight)\n",
            "63it [00:13,  4.52it/s]\n",
            "written features to sentence_length_short_untrained.json\n"
          ]
        }
      ],
      "source": [
        "!python extract_features.py --data_file sentence_length_small.txt --output_file sentence_length_short_untrained.json --untrained_bert"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit classifier on embeddings per layer.\n",
        "Hyperparameters are default config to be comparable to literature"
      ],
      "metadata": {
        "id": "xw_RjbryJZ2O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRlrFZmpw5eb",
        "outputId": "3f0049ce-742b-4f1e-fd00-27ca28eec00b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(dropout=0.0, feats_file='sentence_length_short_untrained.json', labels_file='sentence_length_small.txt', layer=1, nhid=50, seed=123)\n",
            "loaded 432/39/29 samples; 6 labels;\n",
            "best reg = 0.00; dev score = 41.0300; test score = 51.7200;\n",
            "Namespace(dropout=0.0, feats_file='sentence_length_short_untrained.json', labels_file='sentence_length_small.txt', layer=2, nhid=50, seed=123)\n",
            "loaded 432/39/29 samples; 6 labels;\n",
            "best reg = 0.00; dev score = 43.5900; test score = 51.7200;\n",
            "Namespace(dropout=0.0, feats_file='sentence_length_short_untrained.json', labels_file='sentence_length_small.txt', layer=3, nhid=50, seed=123)\n",
            "loaded 432/39/29 samples; 6 labels;\n",
            "best reg = 0.00; dev score = 43.5900; test score = 55.1700;\n",
            "Namespace(dropout=0.0, feats_file='sentence_length_short_untrained.json', labels_file='sentence_length_small.txt', layer=4, nhid=50, seed=123)\n",
            "loaded 432/39/29 samples; 6 labels;\n",
            "best reg = 0.00; dev score = 43.5900; test score = 58.6200;\n",
            "Namespace(dropout=0.0, feats_file='sentence_length_short_untrained.json', labels_file='sentence_length_small.txt', layer=5, nhid=50, seed=123)\n",
            "loaded 432/39/29 samples; 6 labels;\n",
            "best reg = 0.00; dev score = 43.5900; test score = 58.6200;\n",
            "Namespace(dropout=0.0, feats_file='sentence_length_short_untrained.json', labels_file='sentence_length_small.txt', layer=6, nhid=50, seed=123)\n",
            "loaded 432/39/29 samples; 6 labels;\n",
            "best reg = 0.00; dev score = 43.5900; test score = 55.1700;\n"
          ]
        }
      ],
      "source": [
        "!python classifier.py --labels_file sentence_length_small.txt --feats_file sentence_length_short_untrained.json"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "References: \n",
        "Ganesh  Jawahar,  Benoît  Sagot,  and  Djamé  Seddah.2019a. What does BERT learn about the structure oflanguage?  InProceedings of the 57th Annual Meet-ing of the Association for Computational Linguistics,pages 3651–3657, Florence, Italy. Association forComputational Linguistics."
      ],
      "metadata": {
        "id": "YBrRMkDbNUHS"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ADL.inynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}